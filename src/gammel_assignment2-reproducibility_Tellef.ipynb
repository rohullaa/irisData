{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "944d2da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# Run this if on windows to improve speed on windows \n",
    "# https://github.com/intel/scikit-learn-intelex\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a24d1fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5292a5",
   "metadata": {},
   "source": [
    "## Machine learning algorithm\n",
    "### Support vector classifier\n",
    "\n",
    "I chose a support vector classifier, because support vector machine was one of the algorithms we covered in IN3120 Search Technology\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "The hyperparameters of an SVC can be found at the following link:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html?highlight=svc#sklearn.svm.SVC\n",
    "\n",
    "Some main hyperparameters are C, gamma, kernel and degree (for polynomial kernel)\n",
    "\n",
    "C is a number that decides how much leeway you give the algorithm, i.e how many elements can be on the wrong side of the decision boundary\n",
    "\n",
    "Kernel is which kernel to use, e.g. \"poly\" and \"linear\", and gamma is the kernel coefficient. \n",
    "\n",
    "Degree for a polynomial kernel is the degree of the polynomial (and a linear kernel is basically the same as a polynomial kernel of degree 1).  \n",
    "\n",
    "## Dataset\n",
    "### Balance Scale\n",
    "I chose the balance scale dataset because it didn't have too many parameters or instances, so training it wouldn't take too much time for this quick cross validation. The data set can be found at https://archive.ics.uci.edu/ml/datasets/Balance+Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e8308b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B</th>\n",
       "      <th>1</th>\n",
       "      <th>1.1</th>\n",
       "      <th>1.2</th>\n",
       "      <th>1.3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>B</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>624 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     B  1  1.1  1.2  1.3\n",
       "0    R  1    1    1    2\n",
       "1    R  1    1    1    3\n",
       "2    R  1    1    1    4\n",
       "3    R  1    1    1    5\n",
       "4    R  1    1    2    1\n",
       "..  .. ..  ...  ...  ...\n",
       "619  L  5    5    5    1\n",
       "620  L  5    5    5    2\n",
       "621  L  5    5    5    3\n",
       "622  L  5    5    5    4\n",
       "623  B  5    5    5    5\n",
       "\n",
       "[624 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70020738",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.values[:,1:].astype(np.uint8)\n",
    "y = df.values[:,0].astype(str)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5400fa69",
   "metadata": {},
   "source": [
    "### Grid search with cross validation\n",
    "On this link https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-and-model-selection\n",
    "it is noted that cross validation can be used directly with Grid Search to find the optimal hyperparameters, so that's why I chose to try that for this assignment.\n",
    "\n",
    "The hyperparameters i want to vary are the ones specified below, i.e kernel, C and Gamma, and the values are listed in the below list `tuned_parameters`. Since this is an exhaustive search all `3 * 4 * 3 = 36` possible combinations will be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ee516d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for grid search and cross validation I've used this tutorial\n",
    "# https://scikit-learn.org/stable/modules/grid_search.html\n",
    "\n",
    "tuned_parameters = [{'kernel': ['linear', 'poly', 'rbf'],\n",
    "                     'C': [1, 10, 100, 1000],\n",
    "                     'gamma': [.1, .01, 1e-3]}, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94290dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 100, 'gamma': 0.1, 'kernel': 'poly'}, score: 0.9792\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.790 (+/-0.230) for {'C': 1, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "0.859 (+/-0.189) for {'C': 1, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "0.649 (+/-0.019) for {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.790 (+/-0.230) for {'C': 1, 'gamma': 0.01, 'kernel': 'linear'}\n",
      "0.620 (+/-0.058) for {'C': 1, 'gamma': 0.01, 'kernel': 'poly'}\n",
      "0.642 (+/-0.021) for {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.790 (+/-0.230) for {'C': 1, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "0.333 (+/-0.000) for {'C': 1, 'gamma': 0.001, 'kernel': 'poly'}\n",
      "0.639 (+/-0.020) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.863 (+/-0.213) for {'C': 10, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "0.954 (+/-0.075) for {'C': 10, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "0.786 (+/-0.142) for {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.863 (+/-0.213) for {'C': 10, 'gamma': 0.01, 'kernel': 'linear'}\n",
      "0.639 (+/-0.015) for {'C': 10, 'gamma': 0.01, 'kernel': 'poly'}\n",
      "0.655 (+/-0.014) for {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.863 (+/-0.213) for {'C': 10, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "0.333 (+/-0.000) for {'C': 10, 'gamma': 0.001, 'kernel': 'poly'}\n",
      "0.641 (+/-0.025) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.917 (+/-0.055) for {'C': 100, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "0.979 (+/-0.058) for {'C': 100, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "0.846 (+/-0.111) for {'C': 100, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.917 (+/-0.055) for {'C': 100, 'gamma': 0.01, 'kernel': 'linear'}\n",
      "0.674 (+/-0.071) for {'C': 100, 'gamma': 0.01, 'kernel': 'poly'}\n",
      "0.816 (+/-0.160) for {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.917 (+/-0.055) for {'C': 100, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "0.333 (+/-0.000) for {'C': 100, 'gamma': 0.001, 'kernel': 'poly'}\n",
      "0.655 (+/-0.014) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.917 (+/-0.055) for {'C': 1000, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "0.979 (+/-0.058) for {'C': 1000, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "0.846 (+/-0.111) for {'C': 1000, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.917 (+/-0.055) for {'C': 1000, 'gamma': 0.01, 'kernel': 'linear'}\n",
      "0.859 (+/-0.189) for {'C': 1000, 'gamma': 0.01, 'kernel': 'poly'}\n",
      "0.931 (+/-0.155) for {'C': 1000, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.917 (+/-0.055) for {'C': 1000, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "0.620 (+/-0.058) for {'C': 1000, 'gamma': 0.001, 'kernel': 'poly'}\n",
      "0.806 (+/-0.192) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       1.00      0.96      0.98        24\n",
      "           L       0.99      1.00      1.00       104\n",
      "           R       1.00      1.00      1.00       122\n",
      "\n",
      "    accuracy                           1.00       250\n",
      "   macro avg       1.00      0.99      0.99       250\n",
      "weighted avg       1.00      1.00      1.00       250\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 100, 'gamma': 0.1, 'kernel': 'poly'}, score: 0.9634\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.770 (+/-0.194) for {'C': 1, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "0.849 (+/-0.168) for {'C': 1, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "0.628 (+/-0.020) for {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.770 (+/-0.194) for {'C': 1, 'gamma': 0.01, 'kernel': 'linear'}\n",
      "0.602 (+/-0.052) for {'C': 1, 'gamma': 0.01, 'kernel': 'poly'}\n",
      "0.621 (+/-0.016) for {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.770 (+/-0.194) for {'C': 1, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "0.220 (+/-0.002) for {'C': 1, 'gamma': 0.001, 'kernel': 'poly'}\n",
      "0.620 (+/-0.017) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.804 (+/-0.174) for {'C': 10, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "0.955 (+/-0.083) for {'C': 10, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "0.786 (+/-0.142) for {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.804 (+/-0.174) for {'C': 10, 'gamma': 0.01, 'kernel': 'linear'}\n",
      "0.619 (+/-0.012) for {'C': 10, 'gamma': 0.01, 'kernel': 'poly'}\n",
      "0.635 (+/-0.015) for {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.804 (+/-0.174) for {'C': 10, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "0.220 (+/-0.002) for {'C': 10, 'gamma': 0.001, 'kernel': 'poly'}\n",
      "0.621 (+/-0.020) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.867 (+/-0.093) for {'C': 100, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "0.963 (+/-0.064) for {'C': 100, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "0.833 (+/-0.153) for {'C': 100, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.867 (+/-0.093) for {'C': 100, 'gamma': 0.01, 'kernel': 'linear'}\n",
      "0.673 (+/-0.105) for {'C': 100, 'gamma': 0.01, 'kernel': 'poly'}\n",
      "0.804 (+/-0.149) for {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.867 (+/-0.093) for {'C': 100, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "0.220 (+/-0.002) for {'C': 100, 'gamma': 0.001, 'kernel': 'poly'}\n",
      "0.634 (+/-0.014) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.867 (+/-0.093) for {'C': 1000, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "0.963 (+/-0.064) for {'C': 1000, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "0.833 (+/-0.153) for {'C': 1000, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.867 (+/-0.093) for {'C': 1000, 'gamma': 0.01, 'kernel': 'linear'}\n",
      "0.849 (+/-0.168) for {'C': 1000, 'gamma': 0.01, 'kernel': 'poly'}\n",
      "0.908 (+/-0.125) for {'C': 1000, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.867 (+/-0.093) for {'C': 1000, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "0.602 (+/-0.052) for {'C': 1000, 'gamma': 0.001, 'kernel': 'poly'}\n",
      "0.794 (+/-0.123) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       1.00      0.96      0.98        24\n",
      "           L       0.99      1.00      1.00       104\n",
      "           R       1.00      1.00      1.00       122\n",
      "\n",
      "    accuracy                           1.00       250\n",
      "   macro avg       1.00      0.99      0.99       250\n",
      "weighted avg       1.00      1.00      1.00       250\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The code here is from\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
    "\n",
    "scores = ['recall', 'f1']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        SVC(), tuned_parameters, scoring='%s_macro' % score\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(f\"{clf.best_params_}, score: {clf.best_score_:.4f}\")\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    \n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d460d159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9913129050867013\n",
      "{'C': 100, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "0.996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.985574193548387,\n",
       " array([0.976     , 0.968     , 1.        , 1.        , 0.98387097]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(clf.score(X_test, y_test))\n",
    "best_params = clf.best_params_\n",
    "print(\"Best parameters:\", best_params)\n",
    "clf = SVC(**best_params, random_state=1).fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "clf = SVC(random_state=1, **best_params)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean(), scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f26391",
   "metadata": {},
   "source": [
    "### Best parameters\n",
    "Both for recall and f1, the best parameters given by the code above are `{'C': 100, 'gamma': 0.1, 'kernel': 'poly'}`. The performance is pretty good, 0.996 when trained on the training set and tested on the test set, and 0.986 when tested with cross validation. \n",
    "\n",
    "In reality i expect the performance to be a little lower, because the classifier might be overfit to this data set, which is pretty small, as it only has 625 instances. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
